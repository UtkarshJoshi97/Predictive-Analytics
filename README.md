# MSBA-6421 - Practice-Predictive-Analytics  

## ðŸ“Œ About This Repository  
This repository is a collection of Jupyter Notebooks documenting my **hands-on learning** of various **Machine Learning techniques**. Each notebook explores a different ML concept with practical implementation.  

---

# ðŸ“’ Notebooks Summary â€“ Predictive Analytics Projects

---

## ðŸ“˜ Notebook 1: Feature Selection & Hyperparameter Tuning

This notebook dives into the nitty-gritty of fine-tuning model performance through careful feature selection and hyperparameter optimization.

### Techniques Covered:
- **Recursive Feature Elimination (RFE)** â€“ Prunes your features like a bonsai master, keeping only what matters.
- **GridSearchCV** â€“ Hyperparameter tuning made lazy â€” let the machine try all combinations for you.
- **K-Fold Cross-Validation** â€“ Gives your model a real stress test across multiple data splits, ensuring itâ€™s not just good on one lucky fold.

---

## ðŸ“™ Notebook 2: Ordinal Variable Treatment & Model Performance Analysis

Explores how to handle ordinal variables and their impact on model performance â€” numeric vs categorical â€” and goes deep on evaluation.

### Techniques Covered:
- **Numeric vs Categorical Treatment** â€“ Side-by-side comparison of different encoding strategies for ordinal variables. Because how you treat your data matters.
- **Nested Cross-Validation** â€“ The gold standard for model evaluation, nesting CV loops like a Russian doll.
- **Learning Curve Analysis** â€“ Plots that tell you if your modelâ€™s getting smarter or just memorizing flashcards.
- **GridSearchCV** â€“ Once again, because good tuning deserves repetition.
- **Model Evaluation** â€“ Final testing on a 20% holdout dataset to check if your model generalizes or flops under pressure.

---

## ðŸ“• Notebook 3: Predicting Customer Spending 

This notebook tackles customer spending prediction using a variety of regression techniques, applied to both the full dataset and a restricted subset where purchases actually occurred. The goal: model how much a customer is likely to spend â€” if anything â€” and compare modeling strategies across scenarios.

### Techniques Covered:
- **Linear Regression, k-NN, Regression Trees, SVM Regression, Neural Networks, and Ensembles** â€“ A buffet of models to see what works best.
- **Hyperparameter Tuning** â€“ Tweaking knobs like a DJ to get the cleanest predictive sound.
- **Feature Normalization** â€“ Keeping it fair across scales, especially for distance-based models.
- **Comparison of Full vs Purchase-Only Data** â€“ Analyzing performance differences when modeling for all customers vs only buyers.
- **Model Interpretation & Evaluation** â€“ Talking metrics: RMSE, MAE, RÂ², and which models actually generalize well.

ðŸŽ¯ **Outcome:** Insights into which algorithms perform best for numeric prediction in noisy real-world data, and how model behavior shifts when restricted to actual buyers.

---

## ðŸ“— Notebook 3: Spam Detection & Cost-Sensitive Classification 

This notebook focuses on spam email classification, exploring both plain accuracy-focused models and those built with **cost sensitivity** in mind (because in the real world, not all mistakes cost the same).

### Techniques Covered:
- **Multiple Classification Models** â€“ Logistic Regression, k-NN, Naive Bayes, Decision Trees, SVM, Random Forests, etc. (aka the whole squad).
- **Hyperparameter Tuning** â€“ Optimizing model configs via GridSearchCV and cross-validation.
- **Normalization** â€“ Because the dataset is spicy with varying scales.
- **Cost-Sensitive Learning** â€“ Using a 10:1 misclassification cost ratio (false negatives are way more expensive).
- **Model Evaluation with Metrics** â€“ Accuracy, Precision, Recall, F1, AUC, Misclassification Cost.
- **Visualizations** â€“ ROC curves, lift charts, confusion matricesâ€”bringing model performance to life.
- **Nested Cross-Validation** â€“ Preventing overfitting while tuning for real-world cost impact.

ðŸŽ¯ **Outcome:** A deep dive into not just which models perform well, but which ones are most cost-effective when it matters most (like avoiding missing a spam bomb in your inbox).

---

ðŸ“Œ *More notebooks to come as I keep exploring this wild world of predictive modeling!*

